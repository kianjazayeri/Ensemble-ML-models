>> % Convert the response variable to double
response = double(Total_out(:, 1));

% Split the data into training and testing sets
train_size = round(size(Total_in, 1) * 0.8);
train_in = Total_in(1:train_size, :);
train_out = response(1:train_size, :);
test_in = Total_in(train_size+1:end, :);
test_out = response(train_size+1:end, :);

% Set hyperparameters to try
epsilons = [0.01, 0.1, 1];
kernel_scales = [0.1, 1, 10];
box_constraints = [0.1, 1, 10, 100];

% Set k-fold cross-validation parameters
num_folds = 5;
cv = cvpartition(size(train_in, 1), 'KFold', num_folds);

best_mae_svr = Inf;
for epsilon = epsilons
    for kernel_scale = kernel_scales
        for box_constraint = box_constraints
            mae_svr_temp_sum = 0;
            for i = 1:num_folds
                train_in_fold = train_in(cv.training(i), :);
                train_out_fold = train_out(cv.training(i));
                test_in_fold = train_in(cv.test(i), :);
                test_out_fold = train_out(cv.test(i));

                svr_model_temp = fitrsvm(train_in_fold, train_out_fold, 'Epsilon', epsilon, 'KernelScale', kernel_scale, 'BoxConstraint', box_constraint);
                svr_predictions_temp = predict(svr_model_temp, test_in_fold);
                mae_svr_temp = mean(abs(test_out_fold - svr_predictions_temp));
                mae_svr_temp_sum = mae_svr_temp_sum + mae_svr_temp;
            end
            mae_svr_temp_avg = mae_svr_temp_sum / num_folds;
            if mae_svr_temp_avg < best_mae_svr
                best_mae_svr = mae_svr_temp_avg;
                best_hyperparams_svr = [epsilon, kernel_scale, box_constraint];
            end
        end
    end
end

% Train the final SVR model with the best hyperparameters
best_epsilon = best_hyperparams_svr(1);
best_kernel_scale = best_hyperparams_svr(2);
best_box_constraint = best_hyperparams_svr(3);
svr_model = fitrsvm(train_in, train_out, 'Epsilon', best_epsilon, 'KernelScale', best_kernel_scale, 'BoxConstraint', best_box_constraint);

% Make predictions on the test data using the best SVR model
svr_predictions = predict(svr_model, test_in);

% Evaluate the performance of the SVR model
mse_svr = mean((test_out - svr_predictions).^2);
mae_svr = mean(abs(test_out - svr_predictions));

% Set random forest parameters
num_trees = 100;

% Set hyperparameters to try
min_leaf_sizes = [1, 5, 10, 20];
max_num_splits = [10, 20, 50, 100];
num_vars_to_sample = [5, 10, 20, 50];

% Set k-fold cross-validation parameters
num_folds = 5;
cv = cvpartition(size(train_in, 1), 'KFold', num_folds);

best_mae = Inf;
for min_leaf_size = min_leaf_sizes
    for max_num_split = max_num_splits
        for num_vars = num_vars_to_sample
            mae_rf_temp_sum = 0;
            for i = 1:num_folds
                train_in_fold = train_in(cv.training(i), :);
                train_out_fold = train_out(cv.training(i));
                test_in_fold = train_in(cv.test(i), :);
                test_out_fold = train_out(cv.test(i));

                Mdl_rf_temp = TreeBagger(num_trees, train_in_fold, train_out_fold, 'Method', 'regression', ...
                    'MinLeafSize', min_leaf_size, 'MaxNumSplits', max_num_split, ...
                    'NumVariablesToSample', num_vars);
                predicted_out_rf_temp = predict(Mdl_rf_temp, test_in_fold);
                mae_rf_temp = mean(abs(test_out_fold - predicted_out_rf_temp));
                mae_rf_temp_sum = mae_rf_temp_sum + mae_rf_temp;
            end
            mae_rf_temp_avg = mae_rf_temp_sum / num_folds;
            if mae_rf_temp_avg < best_mae
                best_mae = mae_rf_temp_avg;
                best_hyperparams = [min_leaf_size, max_num_split, num_vars];
            end
        end
    end
end

% Train the final random forest model with the best hyperparameters
best_min_leaf_size = best_hyperparams(1);
best_max_num_split = best_hyperparams(2);
best_num_vars = best_hyperparams(3);
Mdl_rf = TreeBagger(num_trees, train_in, train_out, 'Method', 'regression', ...
    'MinLeafSize', best_min_leaf_size, 'MaxNumSplits', best_max_num_split, ...
    'NumVariablesToSample', best_num_vars);

% Make predictions on the test data using the best random forest model
predicted_out_rf = predict(Mdl_rf, test_in);

% Evaluate the performance of the random forest model
mse_rf = mean((test_out - predicted_out_rf).^2);
mae_rf = mean(abs(test_out - predicted_out_rf));

% Ensemble of SVR and Random Forest

% Combine the predictions
predicted_out_combined = [svr_predictions, predicted_out_rf];

% Bagging Ensemble
ens_bagging = fitensemble(predicted_out_combined, test_out, 'Bag', 200, 'Tree', 'Type', 'Regression');
predicted_out_bagging = predict(ens_bagging, predicted_out_combined);
mse_bagging = mean((test_out - predicted_out_bagging).^2);
mae_bagging = mean(abs(test_out - predicted_out_bagging));

% Boosting Ensemble
ens_boosting = fitensemble(predicted_out_combined, test_out, 'LSBoost', 200, 'Tree', 'Type', 'Regression');
predicted_out_boosting = predict(ens_boosting, predicted_out_combined);
mse_boosting = mean((test_out - predicted_out_boosting).^2);
mae_boosting = mean(abs(test_out - predicted_out_boosting));

% Weighted Average Ensemble
wa_ens = (svr_predictions * 0.5) + (predicted_out_rf * 0.5);
mse_wa_ens = mean((test_out - wa_ens).^2);
mae_wa_ens = mean(abs(test_out - wa_ens));

% Combine the predictions into a matrix
predicted_out_combined = [svr_predictions, predicted_out_rf, predicted_out_bagging, predicted_out_boosting];

% Train a linear regression model to combine the predictions
stacked_model = fitlm(predicted_out_combined, test_out);
predicted_out_stacked = predict(stacked_model, predicted_out_combined);
mse_stacked = mean((test_out - predicted_out_stacked).^2);
mae_stacked = mean(abs(test_out - predicted_out_stacked));

% Calculate MAPE
mape_svr = mean(abs((test_out - svr_predictions) ./ test_out)) * 100;
mape_rf = mean(abs((test_out - predicted_out_rf) ./ test_out)) * 100;
mape_bagging = mean(abs((test_out - predicted_out_bagging) ./ test_out)) * 100;
mape_boosting = mean(abs((test_out - predicted_out_boosting) ./ test_out)) * 100;
mape_wa_ens = mean(abs((test_out - wa_ens) ./ test_out)) * 100;
mape_stacked = mean(abs((test_out - predicted_out_stacked) ./ test_out)) * 100;

mse_svr = mean((test_out - svr_predictions).^2);
mse_rf = mean((test_out - predicted_out_rf).^2);
mse_bagging = mean((test_out - predicted_out_bagging).^2);
mse_boosting = mean((test_out - predicted_out_boosting).^2);
mse_wa_ens = mean((test_out - wa_ens).^2);
mse_stacked = mean((test_out - predicted_out_stacked).^2);

% Display the performance indicators
fprintf('SVR Mean Squared Error: %.6f\n', mse_svr);
fprintf('SVR Mean Absolute Error: %.6f\n', mae_svr);
fprintf('SVR Mean Absolute Percentage Error: %.6f%%\n', mape_svr);
fprintf('Random Forest Mean Squared Error: %.6f\n', mse_rf);
fprintf('Random Forest Mean Absolute Error: %.6f\n', mae_rf);
fprintf('Random Forest Mean Absolute Percentage Error: %.6f%%\n', mape_rf);
fprintf('Bagging Ensemble Model Mean Squared Error: %.6f\n', mse_bagging);
fprintf('Bagging Ensemble Model Mean Absolute Error: %.6f\n', mae_bagging);
fprintf('Bagging Ensemble Model Mean Absolute Percentage Error: %.6f%%\n', mape_bagging);
fprintf('Boosting Ensemble Model Mean Squared Error: %.6f\n', mse_boosting);
fprintf('Boosting Ensemble Model Mean Absolute Error: %.6f\n', mae_boosting);
fprintf('Boosting Ensemble Model Mean Absolute Percentage Error: %.6f%%\n', mape_boosting);
fprintf('Weighted Average Ensemble Model Mean Squared Error: %.6f\n', mse_wa_ens);
fprintf('Weighted Average Ensemble Model Mean Absolute Error: %.6f\n', mae_wa_ens);
fprintf('Weighted Average Ensemble Model Mean Absolute Percentage Error: %.6f%%\n', mape_wa_ens);
fprintf('Stacked Ensemble Model Mean Squared Error: %.6f\n', mse_stacked);
fprintf('Stacked Ensemble Model Mean Absolute Error:%.6f\n', mae_stacked);
fprintf('Stacked Ensemble Model Mean Absolute Percentage Error: %.6f%%\n', mape_stacked);

% Plot the actual and predicted values for the neural network
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(predicted_out_nn', 'LineWidth', 2, 'Color', 'red');
xlabel('Sample index');
ylabel('Target value');
title('Neural Network: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the decision tree
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(predicted_out_tree, 'LineWidth', 2, 'Color', 'green');
xlabel('Sample index');
ylabel('Target value');
title('Decision Tree: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the bagging ensemble model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(predicted_out_bagging, 'LineWidth', 2, 'Color', 'magenta');
xlabel('Sample index');
ylabel('Target value');
title('Bagging Ensemble: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the boosting ensemble model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(predicted_out_boosting, 'LineWidth', 2, 'Color', 'black');
xlabel('Sample index');
ylabel('Target value');
title('Boosting Ensemble: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the weighted average ensemble model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(wa_ens, 'LineWidth', 2, 'Color', 'cyan');
xlabel('Sample index');
ylabel('Target value');
title('Weighted Average Ensemble: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the stacked ensemble model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(predicted_out_stacked, 'LineWidth', 2, 'Color', 'yellow');
xlabel('Sample index');
ylabel('Target value');
title('Stacked Ensemble: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the SVR model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(svr_predictions, 'LineWidth', 2, 'Color', 'green');
xlabel('Sample index');
ylabel('Target value');
title('SVR Model: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the random forest model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(predicted_out_rf, 'LineWidth', 2, 'Color', 'orange');
xlabel('Sample index');
ylabel('Target value');
title('Random Forest Model: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the bagging ensemble model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(predicted_out_bagging, 'LineWidth', 2, 'Color', 'magenta');
xlabel('Sample index');
ylabel('Target value');
title('Bagging Ensemble: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the boosting ensemble model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(predicted_out_boosting, 'LineWidth', 2, 'Color', 'black');
xlabel('Sample index');
ylabel('Target value');
title('Boosting Ensemble: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the weighted average ensemble model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(wa_ens, 'LineWidth', 2, 'Color', 'cyan');
xlabel('Sample index');
ylabel('Target value');
title('Weighted Average Ensemble: Actual vs. Predicted');
legend('Actual', 'Predicted');

% Plot the actual and predicted values for the stacked ensemble model
figure;
plot(test_out, 'LineWidth', 2, 'Color', 'blue');
hold on;
plot(predicted_out_stacked, 'LineWidth', 2, 'Color', 'yellow');
xlabel('Sample index');
ylabel('Target value');
title('Stacked Ensemble: Actual vs. Predicted');
legend('Actual', 'Predicted');
